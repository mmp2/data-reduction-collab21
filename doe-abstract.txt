Objective: GigaMan, a non-linear dimension reduction toolbox in python, that scales to data matrices of order n x p ~ 10^12, by vertically incorporating the state of the art theoretical results in the mathematics, statistitics and computer science of geometric learning in high dimensions. Specifically, the bottleneck to scaling non-linear embedding algorithms to large data sets resides in finding the nearest neighbors (NN) of each data point. The core of our methodological contributions are as follows. First, to prove mathematically that the modern NN methods in CS lead to statistically consistent results when used in conjunction with embedding algorithms. Second, to make the existing statistical guarantees and error bounds for this algorithms relevant on actual data. The challenge is that all error bounds and guarantees depend on undefined constants and on parameters that depend on the data distribution. Hence the task is to define statistically grounded methods to measure and test the data properties, and from them to derive uncertainty quantifications useable by the scientist. Around this foundations, we will design embedding methods specialized to the data statistics of various data rich domains such as cosmology, astronomy, and theoretical chemistry. All the methods will be released in the open-source package GigaMan. In collaboration with Argonne National Labs, the package will be implemented on the LCF architectures Aurora and Polaris. 
